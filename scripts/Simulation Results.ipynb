{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import glob\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import json\n",
    "from asreview import open_state\n",
    "from asreviewcontrib.insights.plot import plot_recall\n",
    "from asreviewcontrib.insights.plot import _recall_values\n",
    "from asreviewcontrib.insights.utils import pad_simulation_labels\n",
    "import asreviewcontrib.insights.metrics as met\n",
    "from langdetect import detect\n",
    "import pylab\n",
    "import shutil\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move state files from simulations into state_files folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../simulations/state_files' \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_source = \"../simulations/prior-knowledge-{}/output/simulation/Hamilton_{}/state_files/sim_Hamilton_{}_{}_{}_0.asreview\"\n",
    "state_target = \"../simulations/state_files/pk{}_sim_Hamilton_{}_{}_{}_0.asreview\"\n",
    "prior_knowledge = ['1','2']\n",
    "classifiers = ['logistic', 'svm']\n",
    "datasets = ['Original', 'English', 'Multi_1', 'Multi_2']\n",
    "models = ['tfidf', 'sbert', 'mbert', 'muse', 'mlongt5', 'labse', 'laser', 'mpnet', 'minilm', 'stsb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prior in prior_knowledge:\n",
    "    for classifier in classifiers:\n",
    "        for dataset in datasets:\n",
    "            for model in models:\n",
    "                source = state_source.format(prior, dataset, dataset, classifier, model)\n",
    "                target = state_target.format(prior, dataset, classifier, model)\n",
    "                if (os.path.exists(source)) & (not os.path.exists(target)):\n",
    "                    shutil.copy(source, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract statefiles into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame()\n",
    "for prior in prior_knowledge:\n",
    "    for classifier in classifiers:\n",
    "        for dataset in datasets:\n",
    "            for model in models:\n",
    "                row = {'Dataset':dataset, 'Classifier':classifier, 'Model':model, 'Prior':prior}\n",
    "                with open_state(state_target.format(prior, dataset, classifier, model)) as state:\n",
    "                    metrics = met.get_metrics(state)\n",
    "                    for item in metrics['data']['items']:\n",
    "                        if item.get('title') == 'Time to discovery':\n",
    "                            for paper_td in item.get('value'):\n",
    "                                if paper_td[0] in [300, 567, 741, 878, 1112]:\n",
    "                                    row['td_'+str(paper_td[0])] = paper_td[1]\n",
    "                        elif type(item.get('value')) != list:\n",
    "                            row[item.get('title')] = item.get('value')\n",
    "                        else:\n",
    "                            for value in item.get('value'):\n",
    "                                row[str(item.get('title'))+'_'+str(value[0])] = value[1]\n",
    "                metrics_df = metrics_df.append(row, ignore_index = True)\n",
    "metrics_df = metrics_df.rename(columns={\"Average time to discovery\": \"ATD\", \"Work Saved over Sampling_0.95\": \"WSS@95\"})\n",
    "metrics_df['ctd_targets'] = metrics_df['td_300'] + metrics_df['td_567'] + metrics_df['td_741'] + metrics_df['td_878'] + metrics_df['td_1112']\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#metrics_df.to_csv(\"metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All simulations\n",
    "sim = metrics_df\n",
    "sim[['Model', 'Dataset', 'Classifier', 'Prior', \n",
    "        'Recall_0.1', 'WSS@95', \n",
    "        'ATD', 'td_1112', 'td_300', 'td_567', 'td_741', 'td_878']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific simulations\n",
    "sim = metrics_df.loc[(metrics_df.Classifier == 'logistic') & (metrics_df.Prior == '1') & (metrics_df.Dataset == 'Multi_2')]\n",
    "sim[['Model', 'Dataset', 'Classifier', 'Prior', \n",
    "        'Recall_0.1', 'WSS@95', \n",
    "        'ATD', 'td_300', 'td_567', 'td_741', 'td_878', 'td_1112', 'td_sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average grouped by dataset\n",
    "sim = metrics_df\n",
    "sim.groupby(['Dataset'])['Recall_0.1', 'WSS@95', 'ATD', 'td_300', 'td_567', 'td_741', 'td_878', 'td_1112', 'td_sum'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average grouped by dataset and classifier\n",
    "sim = metrics_df\n",
    "sim.groupby(['Dataset', 'Classifier'])['Recall_0.1', 'WSS@95', 'ATD', 'td_300', 'td_567', 'td_741', 'td_878', 'td_1112', 'td_sum'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average grouped by dataset and prior knowledge\n",
    "sim = metrics_df\n",
    "sim.groupby(['Dataset', 'Prior'])['Recall_0.1', 'WSS@95', 'ATD', 'td_300', 'td_567', 'td_741', 'td_878', 'td_1112', 'td_sum'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average grouped by dataset and model\n",
    "sim = metrics_df\n",
    "sim.groupby(['Dataset', 'Model'])['Recall_0.1', 'WSS@95', 'ATD', 'td_300', 'td_567', 'td_741', 'td_878', 'td_1112', 'td_sum'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATD Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ATD's of target papers per dataset\n",
    "# Define the mapping dictionary for dataset renaming\n",
    "dataset_mapping = {\n",
    "    'Original': 'Multiple',\n",
    "    'Multi_1': 'Non-English 1',\n",
    "    'Multi_2': 'Non-English 2'\n",
    "}\n",
    "\n",
    "# Group by the desired factor and calculate the mean of 'td' values\n",
    "grouped_df = metrics_df.groupby('Dataset')['td_1112', 'td_300', 'td_567', 'td_741', 'td_878'].mean()\n",
    "\n",
    "# Sort the grouped DataFrame by the average 'td' values in ascending order\n",
    "grouped_df = grouped_df.reindex(grouped_df.mean(axis=1).sort_values().index)\n",
    "\n",
    "# Set the factor you want to group by (e.g., 'Dataset', 'Prior Knowledge', 'Classifier')\n",
    "group_by_factor = 'Dataset'\n",
    "\n",
    "# Create the stacked bar chart\n",
    "ax = grouped_df.plot.bar(stacked=True)\n",
    "\n",
    "# Set the x-axis label\n",
    "plt.xlabel(group_by_factor)\n",
    "\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Average Time to Discovery')\n",
    "\n",
    "# Modify the legend labels\n",
    "legend_labels = ['Record 1112', 'Record 300', 'Record 567', 'Record 741', 'Record 878']\n",
    "ax.legend(labels=legend_labels, bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Rename the datasets on the x-axis\n",
    "new_labels = [dataset_mapping.get(label.get_text(), label.get_text()) for label in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "# Set the title for the plot\n",
    "plt.title(\"Cumulative Average Time to Discovery (CATD) of Target Papers per Dataset\")\n",
    "\n",
    "# Add the sum of the entire bar on top\n",
    "for i, value in enumerate(grouped_df.values):\n",
    "    x = i\n",
    "    y = value.sum()\n",
    "    \n",
    "    # Annotate the rounded sum value at the top of each bar\n",
    "    ax.annotate(f'{int(round(y))}', (x, y), xytext=(0, 3), textcoords='offset points',\n",
    "                ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "# Adjust the y-axis limits\n",
    "ax.set_ylim(top=ax.get_ylim()[1] * 1.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the mapping dictionary for dataset renaming\n",
    "dataset_mapping = {\n",
    "    'Original': 'Multiple',\n",
    "    'Multi_1': 'Non-English 1',\n",
    "    'Multi_2': 'Non-English 2'\n",
    "}\n",
    "\n",
    "groups = ['Classifier', 'Prior']\n",
    "\n",
    "for group in groups:\n",
    "    # Group by the desired factor and calculate the mean of 'td' values\n",
    "    grouped_df = metrics_df.groupby(['Dataset', group])['td_1112', 'td_300', 'td_567', 'td_741', 'td_878'].mean()\n",
    "\n",
    "    # Set the factor you want to group by (e.g., 'Dataset', 'Prior Knowledge', 'Classifier')\n",
    "    group_by_factor = 'Dataset'\n",
    "\n",
    "    # Create the stacked bar chart\n",
    "    ax = grouped_df.plot.bar(stacked=True)\n",
    "\n",
    "    # Set the x-axis label\n",
    "    plt.xlabel(group_by_factor + ' + ' + group)\n",
    "\n",
    "    # Set the y-axis label\n",
    "    plt.ylabel('Average Time to Discovery')\n",
    "\n",
    "    # Modify the legend labels\n",
    "    legend_labels = ['Record 1112', 'Record 300', 'Record 567', 'Record 741', 'Record 878']\n",
    "    ax.legend(labels=legend_labels, bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Update the dataset labels on the x-axis to include prior knowledge\n",
    "    x_pos = range(len(grouped_df))\n",
    "    xticklabels = [\n",
    "        f\"{dataset_mapping.get(dataset, dataset)}, {prior.split('_')[-1]}\"\n",
    "        for (dataset, prior), i in zip(grouped_df.index, x_pos)\n",
    "    ]\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(xticklabels, rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "    # Set the title for the plot\n",
    "    plt.title(\"Cumulative Average Time to Discovery (CATD) of Target Papers \\n per Dataset and \" + group)\n",
    "\n",
    "    # Add the sum of the entire bar on top\n",
    "    for i, value in enumerate(grouped_df.values):\n",
    "        x = i\n",
    "        y = value.sum()\n",
    "\n",
    "        # Annotate the rounded sum value at the top of each bar\n",
    "        ax.annotate(f'{int(round(y))}', (x, y), xytext=(0, 3), textcoords='offset points',\n",
    "                    ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "    # Adjust the y-axis limits\n",
    "    ax.set_ylim(top=ax.get_ylim()[1] * 1.1)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATD's per model per dataset\n",
    "# Create a new figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Iterate over each dataset\n",
    "for i, dataset in enumerate(datasets):\n",
    "    # Select the data for the current dataset\n",
    "    df = metrics_df.loc[metrics_df.Dataset == dataset]\n",
    "\n",
    "    # Group by the desired factor and calculate the mean of 'td' values\n",
    "    grouped_df = df.groupby('Model')['td_1112', 'td_300', 'td_567', 'td_741', 'td_878'].mean()\n",
    "\n",
    "    # Sort the grouped DataFrame by the average 'td' values in ascending order (only for 'Model' group)\n",
    "    grouped_df = grouped_df.reindex(grouped_df.mean(axis=1).sort_values().index)\n",
    "\n",
    "    # Create the stacked bar chart for the current dataset\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    grouped_df.plot.bar(stacked=True, ax=ax)\n",
    "\n",
    "    # Set the x-axis label\n",
    "    ax.set_xlabel('Model')\n",
    "\n",
    "    # Set the y-axis label\n",
    "    ax.set_ylabel('Average Time to Discovery')\n",
    "\n",
    "    # Set the title for each plot with labels A, B, C, D\n",
    "    dataset_title = dataset_mapping.get(dataset, dataset)\n",
    "    ax.set_title(f\"({string.ascii_uppercase[i]}) Dataset: {dataset_title}\")\n",
    "\n",
    "    # Add the sum of the entire bar on top\n",
    "    for j, value in enumerate(grouped_df.values):\n",
    "        x = j\n",
    "        y = value.sum()\n",
    "\n",
    "        # Annotate the rounded sum value at the top of each bar\n",
    "        ax.annotate(f'{int(round(y))}', (x, y), xytext=(0, 3), textcoords='offset points',\n",
    "                    ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "    # Adjust the y-axis limits\n",
    "    ax.set_ylim(top=ax.get_ylim()[1] * 1.1)\n",
    "\n",
    "# Set the title for the figure\n",
    "fig.suptitle(\"Cumulative Average Time to Discovery (CATD) of Target Papers per Model and Dataset\", fontsize=16)\n",
    "\n",
    "# Remove legends from subplots\n",
    "for ax in axes.flatten():\n",
    "    ax.legend().remove()\n",
    "\n",
    "# Modify the legend labels\n",
    "legend_labels = ['Record 1112', 'Record 300', 'Record 567', 'Record 741', 'Record 878']\n",
    "\n",
    "# Create a common legend underneath the subplots\n",
    "fig.legend(labels=legend_labels, loc='lower center', title='Records', ncol=5, bbox_to_anchor=(0.5, -0.15))\n",
    "\n",
    "# Adjust the spacing between subplots and legend\n",
    "fig.tight_layout(rect=[0, -0.1, 1, 0.99])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define language for each paper abstract, this could take up to 1 minute\n",
    "csv_file = \"../simulations/prior-knowledge-1/data/Hamilton_{}.csv\"\n",
    "language_dfs = {}\n",
    "for dataset in datasets:\n",
    "    languages = pd.read_csv(csv_file.format(dataset))\n",
    "    languages['language'] = languages['abstract'].dropna().apply(detect)\n",
    "    languages.columns.values[0] = 'record_id'\n",
    "    language_dfs[dataset] = languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create all recall plots\n",
    "plotCount = 0\n",
    "for prior in prior_knowledge:\n",
    "    for classifier in classifiers:\n",
    "        for dataset in datasets:\n",
    "            plt.figure(figsize=(20, 30))\n",
    "            plt.subplots_adjust(hspace=0.25)\n",
    "            if dataset == 'Original':\n",
    "                tit = 'Multiple'\n",
    "            elif dataset == 'Multi_1':\n",
    "                tit = 'Non-English 1'\n",
    "            elif dataset == 'Multi_2':\n",
    "                tit = 'Non-English 2'\n",
    "            else:\n",
    "                tit = 'English'\n",
    "            plt.suptitle(\"Target records in recall plot - \" + tit + ' - Prior Knowledge ' + prior, fontsize=18, y=0.90)\n",
    "            n = 0\n",
    "            for model in models:\n",
    "                ax = plt.subplot(5, 2, n + 1)\n",
    "                with open_state(state_target.format(prior, dataset, classifier, model)) as state:\n",
    "                    states_df = state.get_dataset()\n",
    "\n",
    "                    sim_labels = pad_simulation_labels(state)\n",
    "                    x, y = _recall_values(sim_labels, x_absolute=False, y_absolute=False)\n",
    "\n",
    "                    languages = language_dfs[dataset]\n",
    "                    states_df = states_df.merge(languages[['record_id', 'language']], on='record_id')\n",
    "                    non_english = states_df.loc[((states_df.language != 'en') & (states_df.language.isna() == False)\n",
    "                                                & (states_df.label == 1))]\n",
    "                    targets = states_df.loc[states_df.record_id.isin([300, 567, 741, 878, 1112])]\n",
    "\n",
    "                    for language in targets.language.sort_values().unique():\n",
    "                        temp = targets.loc[targets.language == language]\n",
    "                        ax.scatter([x[i] for i in temp.index], [y[i] for i in temp.index], marker=\"o\", s=50,\n",
    "                                   alpha=0.8)\n",
    "                        for index, row in temp.iterrows():\n",
    "                            ax.text(x[index] - 0.05, y[index] + 0.03, row.record_id, fontsize=9)\n",
    "                    ax.legend(non_english.language.sort_values().unique())\n",
    "\n",
    "                    # draw the plot\n",
    "                    plot_recall(ax, state)\n",
    "\n",
    "                    ax.set_title(classifier + ' - ' + model + ' (' + string.ascii_uppercase[n] + ')')\n",
    "                    n += 1\n",
    "            plotCount += 1\n",
    "            #plt.savefig('Recall plot ' + str(plotCount) +'.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
